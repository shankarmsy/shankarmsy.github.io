<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Shankar's Data Science Blog (linear regression)</title><link>https://shankarmsy.github.io/</link><description></description><atom:link href="https://shankarmsy.github.io/categories/linear-regression.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Sun, 18 Jan 2015 14:31:28 GMT</lastBuildDate><generator>http://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Visually differentiating PCA and Linear Regression</title><link>https://shankarmsy.github.io/posts/pca-vs-lr.html</link><dc:creator>Shankar Muthuswamy</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;I've always been fascinated by the concept of PCA. Considering its wide range of applications and how inherently mathematical the idea is, I feel PCA is one of the pillars of the intersection between Pure Mathematics and Real-world analytics. Besides, the fact that you could think about real data as just raw numbers and then transform it down to something you can visualize and relate to, is extremely powerful and essential in any learning process.&lt;/p&gt;
&lt;p&gt;Just in case you're wondering, Principle Component Analysis (PCA) simply put is a dimensionality reduction technique that can find the combinations of variables that explain the most variance. So you can transform a 1000-feature dataset into 2D so you can visualize it in a plot or you could bring it down to x features where x&amp;lt;&amp;lt;1000 while preserving most of the variance in the data. I've previously explored &lt;a href="https://shankarmsy.github.io/posts/pca-sklearn.html"&gt;Facial image compression and reconstruction using PCA&lt;/a&gt; using scikit-learn.&lt;/p&gt;
&lt;p&gt;In this post I would like to delve into the concept of linearity in Principal Component Analysis.&lt;/p&gt;&lt;p&gt;&lt;a href="https://shankarmsy.github.io/posts/pca-vs-lr.html"&gt;Read moreâ€¦&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>dimensionality reduction</category><category>image compression</category><category>linear regression</category><category>pca</category><category>scikit-learn</category><category>unsupervised learning</category><guid>https://shankarmsy.github.io/posts/pca-vs-lr.html</guid><pubDate>Fri, 12 Dec 2014 17:01:00 GMT</pubDate></item></channel></rss>