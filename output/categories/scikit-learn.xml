<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>It's all in the data (scikit-learn)</title><link>https://shankarmsy.github.io/</link><description></description><atom:link href="https://shankarmsy.github.io/categories/scikit-learn.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Mon, 26 Jan 2015 12:51:00 GMT</lastBuildDate><generator>http://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Visually differentiating PCA and Linear Regression</title><link>https://shankarmsy.github.io/posts/pca-vs-lr.html</link><dc:creator>Shankar Muthuswamy</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;I've always been fascinated by the concept of PCA. Considering its wide range of applications and how inherently mathematical the idea is, I feel PCA is one of the pillars of the intersection between Pure Mathematics and Real-world analytics. Besides, the fact that you could think about real data as just raw numbers and then transform it down to something you can visualize and relate to, is extremely powerful and essential in any learning process.&lt;/p&gt;
&lt;p&gt;Just in case you're wondering, Principle Component Analysis (PCA) simply put is a dimensionality reduction technique that can find the combinations of variables that explain the most variance. So you can transform a 1000-feature dataset into 2D so you can visualize it in a plot or you could bring it down to x features where x&amp;lt;&amp;lt;1000 while preserving most of the variance in the data. I've previously explored &lt;a href="https://shankarmsy.github.io/posts/pca-sklearn.html"&gt;Facial image compression and reconstruction using PCA&lt;/a&gt; using scikit-learn.&lt;/p&gt;
&lt;p&gt;In this post I would like to delve into the concept of linearity in Principal Component Analysis.&lt;/p&gt;&lt;p&gt;&lt;a href="https://shankarmsy.github.io/posts/pca-vs-lr.html"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>dimensionality reduction</category><category>image compression</category><category>linear regression</category><category>pca</category><category>scikit-learn</category><category>unsupervised learning</category><guid>https://shankarmsy.github.io/posts/pca-vs-lr.html</guid><pubDate>Fri, 12 Dec 2014 17:01:00 GMT</pubDate></item><item><title>Predicting Forest Cover Types with Ensemble Learning</title><link>https://shankarmsy.github.io/posts/forest-cover-types.html</link><dc:creator>Shankar Muthuswamy</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This is a documentation of one of my approaches to solving the forest cover type prediction challenge hosted by Kaggle. Feel free to use for your own reference and let me know if you have any suggestions on how I can improve the model :-)&lt;/p&gt;
&lt;p&gt;I found this topic very engaging being a nature lover. Also the features are very friendly and don't require much domain knowledge to explore (and hopefully engineer new features).&lt;/p&gt;
&lt;p&gt;OK let's get started.&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://shankarmsy.github.io/posts/forest-cover-types.html"&gt;Read more…&lt;/a&gt; (30 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blending</category><category>ensemble</category><category>extra trees</category><category>random forests</category><category>scikit-learn</category><category>supervised learning</category><guid>https://shankarmsy.github.io/posts/forest-cover-types.html</guid><pubDate>Tue, 02 Dec 2014 12:15:33 GMT</pubDate></item><item><title>Saving the Titanic with R &amp; IPython</title><link>https://shankarmsy.github.io/posts/saving-titanic-r.html</link><dc:creator>Shankar Muthuswamy</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;The following is an illustration of one of my approaches to solving the Titanic Survival prediction challenge hosted by Kaggle. Below is an excerpt from the competition page.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://shankarmsy.github.io/posts/saving-titanic-r.html"&gt;Read more…&lt;/a&gt; (55 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>R</category><category>conditional trees</category><category>kaggle</category><category>logistic regression</category><category>random forests</category><category>scikit-learn</category><category>supervised learning</category><category>svm</category><category>titanic</category><guid>https://shankarmsy.github.io/posts/saving-titanic-r.html</guid><pubDate>Sun, 23 Nov 2014 10:11:09 GMT</pubDate></item><item><title>Recognizing Hand Written Digits (UCI ML Repo) with Support Vector Machines (SVM)</title><link>https://shankarmsy.github.io/posts/svm-sklearn.html</link><dc:creator>Shankar Muthuswamy</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Support Vector Machines (SVMs) are a powerful supervised learning algorithm used for &lt;strong&gt;classification&lt;/strong&gt; or for &lt;strong&gt;regression&lt;/strong&gt;. SVMs are a &lt;strong&gt;discriminative&lt;/strong&gt; classifier: that is, they draw a boundary between clusters of data. In this post I will demonstrate hand-written digit recognition using the SVC classifier in scikit-learn. We'll make use of the online dataset available in the UCI machine learning repository.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://shankarmsy.github.io/posts/svm-sklearn.html"&gt;Read more…&lt;/a&gt; (10 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>digit recognition</category><category>scikit-learn</category><category>supervised learning</category><category>svm</category><guid>https://shankarmsy.github.io/posts/svm-sklearn.html</guid><pubDate>Wed, 19 Nov 2014 09:00:03 GMT</pubDate></item><item><title>Data Munging with Pandas (Advanced)</title><link>https://shankarmsy.github.io/posts/munging-pandas.html</link><dc:creator>Shankar Muthuswamy</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Pandas is a Python library for doing data analysis. It's really fast and lets you do exploratory work incredibly quickly.&lt;/p&gt;
&lt;p&gt;In this post I will demonstrate some advanced data munging/wrangling concepts with the awesome Pandas. I intend to code more and write less but will add help text as much as possible. Check out the pandas help documentation to learn more.&lt;/p&gt;
&lt;p&gt;OK, let's get started. &lt;/p&gt;&lt;p&gt;&lt;a href="https://shankarmsy.github.io/posts/munging-pandas.html"&gt;Read more…&lt;/a&gt; (17 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>data munging</category><category>pandas</category><category>python</category><category>scikit-learn</category><guid>https://shankarmsy.github.io/posts/munging-pandas.html</guid><pubDate>Mon, 17 Nov 2014 10:03:44 GMT</pubDate></item><item><title>Facial Image Compression and Reconstruction with PCA</title><link>https://shankarmsy.github.io/posts/pca-sklearn.html</link><dc:creator>Shankar Muthuswamy</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Principle Component Analysis (PCA) is a dimension reduction technique that can find the combinations of variables that explain the most variance. In this post I will demonstrate dimensionality reduction concepts including facial image compression and reconstruction using PCA.&lt;/p&gt;
&lt;p&gt;Let's get started. &lt;/p&gt;&lt;p&gt;&lt;a href="https://shankarmsy.github.io/posts/pca-sklearn.html"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>dimensionality reduction</category><category>image compression</category><category>pca</category><category>scikit-learn</category><category>unsupervised learning</category><guid>https://shankarmsy.github.io/posts/pca-sklearn.html</guid><pubDate>Wed, 12 Nov 2014 18:04:24 GMT</pubDate></item><item><title>Dive-in to Pandas (Basic)</title><link>https://shankarmsy.github.io/posts/dive-in-pandas.html</link><dc:creator>Shankar Muthuswamy</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Pandas is a Python library for doing data analysis. It's really fast and lets you do exploratory work incredibly quickly.&lt;/p&gt;
&lt;p&gt;It provides an R-like DataFrame, produces high quality plots with matplotlib, and integrates nicely with other libraries that expect NumPy arrays. In this post, I'll go through the basics of pandas. Check out the (very readable) pandas docs or the pandas cookbook if you want to learn more.&lt;/p&gt;
&lt;p&gt;OK, let's get started.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://shankarmsy.github.io/posts/dive-in-pandas.html"&gt;Read more…&lt;/a&gt; (11 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>data munging</category><category>pandas</category><category>python</category><category>scikit-learn</category><guid>https://shankarmsy.github.io/posts/dive-in-pandas.html</guid><pubDate>Wed, 05 Nov 2014 09:36:11 GMT</pubDate></item></channel></rss>