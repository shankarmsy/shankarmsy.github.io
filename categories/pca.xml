<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Shankar's Data Science Blog (pca)</title><link>https://shankarmsy.github.io/</link><description></description><atom:link href="https://shankarmsy.github.io/categories/pca.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Thu, 08 Jan 2015 10:34:02 GMT</lastBuildDate><generator>http://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Visually differentiating PCA and Linear Regression</title><link>https://shankarmsy.github.io/posts/pca-vs-lr.html</link><dc:creator>Shankar Muthuswamy</dc:creator><description>&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;I've always been fascinated by the concept of PCA. Considering its wide range of applications and how inherently mathematical the idea is, I feel PCA is one of the pillars of the intersection between Pure Mathematics and Real-world analytics. Besides, the fact that you could think about real data as just raw numbers and then transform it down to something you can visualize and relate to, is extremely powerful and essential in any learning process.&lt;/p&gt;
&lt;p&gt;Just in case you're wondering, Principle Component Analysis (PCA) simply put is a dimensionality reduction technique that can find the combinations of variables that explain the most variance. So you can transform a 1000-feature dataset into 2D so you can visualize it in a plot or you could bring it down to x features where x&amp;lt;&amp;lt;1000 while preserving most of the variance in the data. I've previously explored &lt;a href="https://shankarmsy.github.io/posts/pca-sklearn.html"&gt;Facial image compression and reconstruction using PCA&lt;/a&gt; using scikit-learn.&lt;/p&gt;
&lt;p&gt;In this post I would like to delve into the concept of linearity in Principal Component Analysis.&lt;/p&gt;&lt;p&gt;&lt;a href="https://shankarmsy.github.io/posts/pca-vs-lr.html"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>dimensionality reduction</category><category>image compression</category><category>linear regression</category><category>pca</category><category>scikit-learn</category><category>unsupervised learning</category><guid>https://shankarmsy.github.io/posts/pca-vs-lr.html</guid><pubDate>Fri, 12 Dec 2014 17:01:00 GMT</pubDate></item><item><title>Facial Image Compression and Reconstruction with PCA</title><link>https://shankarmsy.github.io/posts/pca-sklearn.html</link><dc:creator>Shankar Muthuswamy</dc:creator><description>&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Principle Component Analysis (PCA) is a dimension reduction technique that can find the combinations of variables that explain the most variance. In this post I will demonstrate dimensionality reduction concepts including facial image compression and reconstruction using PCA.&lt;/p&gt;
&lt;p&gt;Let's get started. &lt;/p&gt;&lt;p&gt;&lt;a href="https://shankarmsy.github.io/posts/pca-sklearn.html"&gt;Read more…&lt;/a&gt; (8 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>dimensionality reduction</category><category>image compression</category><category>pca</category><category>scikit-learn</category><category>unsupervised learning</category><guid>https://shankarmsy.github.io/posts/pca-sklearn.html</guid><pubDate>Wed, 12 Nov 2014 18:04:24 GMT</pubDate></item></channel></rss>